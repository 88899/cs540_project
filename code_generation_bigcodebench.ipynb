{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code generation and test workflow for Bigcodebench\n",
    "\n",
    "This is a example how to use openai api to evaluate the Bigcodebench benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "os.environ['OPENAI_API_KEY'] = \"Your OpenAI API key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "OPENAI_MODEL = \"gpt-4o\"\n",
    "OPENAI_TEMPERATURE = 0\n",
    "OPENAI_TOP_P = 1\n",
    "OPENAI_FREQUENCY_PENALTY = 0\n",
    "OPENAI_PRESENCE_PENALTY = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Bigcodebench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>complete_prompt</th>\n",
       "      <th>instruct_prompt</th>\n",
       "      <th>canonical_solution</th>\n",
       "      <th>code_prompt</th>\n",
       "      <th>test</th>\n",
       "      <th>entry_point</th>\n",
       "      <th>doc_struct</th>\n",
       "      <th>libs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BigCodeBench/0</td>\n",
       "      <td>import itertools\\nfrom random import shuffle\\n...</td>\n",
       "      <td>Calculates the average of the sums of absolute...</td>\n",
       "      <td>permutations = list(itertools.permutations...</td>\n",
       "      <td>import itertools\\nfrom random import shuffle\\n...</td>\n",
       "      <td>import unittest\\nfrom unittest.mock import pat...</td>\n",
       "      <td>task_func</td>\n",
       "      <td>{\"description\": [\"Calculates the average of th...</td>\n",
       "      <td>['random', 'itertools']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BigCodeBench/1</td>\n",
       "      <td>import collections\\nimport random\\nimport stri...</td>\n",
       "      <td>Generate a random string of the specified leng...</td>\n",
       "      <td>if length &lt; 0:\\n        raise ValueError\\n...</td>\n",
       "      <td>import collections\\nimport random\\nimport stri...</td>\n",
       "      <td>import unittest\\nimport string\\nclass TestCase...</td>\n",
       "      <td>task_func</td>\n",
       "      <td>{\"description\": [\"Generate a random string of ...</td>\n",
       "      <td>['collections', 'random', 'string']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BigCodeBench/2</td>\n",
       "      <td>import random\\nimport statistics\\n\\ndef task_f...</td>\n",
       "      <td>Create a dictionary in which keys are random l...</td>\n",
       "      <td>random_dict = {k: [random.randint(0, 100) ...</td>\n",
       "      <td>import random\\nimport statistics\\ndef task_fun...</td>\n",
       "      <td>import unittest\\nclass TestCases(unittest.Test...</td>\n",
       "      <td>task_func</td>\n",
       "      <td>{\"description\": [\"Create a dictionary in which...</td>\n",
       "      <td>['statistics', 'random']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BigCodeBench/3</td>\n",
       "      <td>import random\\nimport numpy as np\\n\\ndef task_...</td>\n",
       "      <td>Create a dictionary where keys are specified l...</td>\n",
       "      <td>random_dict = {k: [random.randint(0, 100) ...</td>\n",
       "      <td>import random\\nimport numpy as np\\ndef task_fu...</td>\n",
       "      <td>import unittest\\n    \\nclass TestCases(unittes...</td>\n",
       "      <td>task_func</td>\n",
       "      <td>{\"description\": [\"Create a dictionary where ke...</td>\n",
       "      <td>['numpy', 'random']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BigCodeBench/4</td>\n",
       "      <td>from collections import Counter\\nimport iterto...</td>\n",
       "      <td>Count the occurrence of each integer in the va...</td>\n",
       "      <td>count_dict = Counter(itertools.chain.from_...</td>\n",
       "      <td>from collections import Counter\\nimport iterto...</td>\n",
       "      <td>import unittest\\nclass TestCases(unittest.Test...</td>\n",
       "      <td>task_func</td>\n",
       "      <td>{\"description\": [\"Count the occurrence of each...</td>\n",
       "      <td>['collections', 'itertools']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          task_id                                    complete_prompt  \\\n",
       "0  BigCodeBench/0  import itertools\\nfrom random import shuffle\\n...   \n",
       "1  BigCodeBench/1  import collections\\nimport random\\nimport stri...   \n",
       "2  BigCodeBench/2  import random\\nimport statistics\\n\\ndef task_f...   \n",
       "3  BigCodeBench/3  import random\\nimport numpy as np\\n\\ndef task_...   \n",
       "4  BigCodeBench/4  from collections import Counter\\nimport iterto...   \n",
       "\n",
       "                                     instruct_prompt  \\\n",
       "0  Calculates the average of the sums of absolute...   \n",
       "1  Generate a random string of the specified leng...   \n",
       "2  Create a dictionary in which keys are random l...   \n",
       "3  Create a dictionary where keys are specified l...   \n",
       "4  Count the occurrence of each integer in the va...   \n",
       "\n",
       "                                  canonical_solution  \\\n",
       "0      permutations = list(itertools.permutations...   \n",
       "1      if length < 0:\\n        raise ValueError\\n...   \n",
       "2      random_dict = {k: [random.randint(0, 100) ...   \n",
       "3      random_dict = {k: [random.randint(0, 100) ...   \n",
       "4      count_dict = Counter(itertools.chain.from_...   \n",
       "\n",
       "                                         code_prompt  \\\n",
       "0  import itertools\\nfrom random import shuffle\\n...   \n",
       "1  import collections\\nimport random\\nimport stri...   \n",
       "2  import random\\nimport statistics\\ndef task_fun...   \n",
       "3  import random\\nimport numpy as np\\ndef task_fu...   \n",
       "4  from collections import Counter\\nimport iterto...   \n",
       "\n",
       "                                                test entry_point  \\\n",
       "0  import unittest\\nfrom unittest.mock import pat...   task_func   \n",
       "1  import unittest\\nimport string\\nclass TestCase...   task_func   \n",
       "2  import unittest\\nclass TestCases(unittest.Test...   task_func   \n",
       "3  import unittest\\n    \\nclass TestCases(unittes...   task_func   \n",
       "4  import unittest\\nclass TestCases(unittest.Test...   task_func   \n",
       "\n",
       "                                          doc_struct  \\\n",
       "0  {\"description\": [\"Calculates the average of th...   \n",
       "1  {\"description\": [\"Generate a random string of ...   \n",
       "2  {\"description\": [\"Create a dictionary in which...   \n",
       "3  {\"description\": [\"Create a dictionary where ke...   \n",
       "4  {\"description\": [\"Count the occurrence of each...   \n",
       "\n",
       "                                  libs  \n",
       "0              ['random', 'itertools']  \n",
       "1  ['collections', 'random', 'string']  \n",
       "2             ['statistics', 'random']  \n",
       "3                  ['numpy', 'random']  \n",
       "4         ['collections', 'itertools']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"bigcode/bigcodebench\")\n",
    "df = pd.DataFrame(ds['v0.1.2'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create local folder for bigcodebench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    task_path = f'dataset/bigcodebench/{row[\"task_id\"].replace(\"/\", \"_\")}'\n",
    "    if not os.path.exists(task_path):\n",
    "        os.makedirs(task_path)\n",
    "    with open(f'{task_path}/test.py', 'w') as f:\n",
    "        f.write('from task_func import *\\n' + row['test'] + '\\n\\nif __name__ == \"__main__\":\\n    unittest.main()')\n",
    "    with open(f'{task_path}/instruction.txt', 'w') as f:\n",
    "        f.write(row['instruct_prompt'])\n",
    "    with open(f'{task_path}/task_func.py', 'w') as f:\n",
    "        f.write(\"\")\n",
    "    with open(f'{task_path}/canonical_solution.py', 'w') as f:\n",
    "        f.write(row['code_prompt'] + row['canonical_solution'])\n",
    "    with open(f'{task_path}/canonical_solution_test.py', 'w') as f:\n",
    "        f.write('from canonical_solution import task_func\\n' + row['test'] + '\\n\\nif __name__ == \"__main__\":\\n    unittest.main()')\n",
    "    with open(f'{task_path}/complete_prompt.py', 'w') as f:\n",
    "        f.write(row['complete_prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API call function wapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt4_beta(client, system_prompt, user_prompt, output_format):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model = OPENAI_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ],\n",
    "        response_format = output_format,\n",
    "        temperature=OPENAI_TEMPERATURE,\n",
    "        top_p=OPENAI_TOP_P,\n",
    "        frequency_penalty=OPENAI_FREQUENCY_PENALTY,\n",
    "        presence_penalty=OPENAI_PRESENCE_PENALTY,\n",
    "        seed=42\n",
    "    )\n",
    "    return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_SYSTEM_PROMPT = '''You are an expert programmer. Your task is to complete a code snippet based on the provided instructions and requirements. You will be given a instruction about the coding task.\n",
    "\n",
    "# Your completed code must:\n",
    "1. Strictly adhere to the described behavior and requirements\n",
    "2. Be clear, concise, and maintainable, following programming best practices.\n",
    "3. Avoid adding unnecessary features or deviating from the described requirements.\n",
    "4. Handle any specified edge cases or constraints appropriately.\n",
    "Ensure your implementation is robust and aligns with the functional requirements derived from the provided information.\n",
    "'''\n",
    "\n",
    "USER_PROMPT = '''\\n# INSTRUCTION:\n",
    "{}\n",
    "\n",
    "According to the information, retrun the complete code.\n",
    "'''\n",
    "\n",
    "class decomposed_instruction(BaseModel):\n",
    "  completed_code: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "target_folder = 'dataset/bigcodebench/BigCodeBench_{}'\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Tasks\"):\n",
    "    task_id = row['task_id'].split('/')[1]\n",
    "    task_folder = target_folder.format(task_id)\n",
    "    instruct_prompt = row['instruct_prompt']\n",
    "    user_instruciton = USER_PROMPT.format(instruct_prompt)\n",
    "    start_time = time.time()\n",
    "    message = generate_gpt4_beta(client, GENERATION_SYSTEM_PROMPT, user_instruciton, decomposed_instruction)\n",
    "    time_taken = time.time() - start_time\n",
    "    if time_taken <= 5:\n",
    "        time.sleep(5 - time_taken)  # wait 5 seconds more to avoid rate limit\n",
    "    with open(f'{task_folder}/task_func.py.', 'w') as file:\n",
    "        file.write('\\n'.join(message.parsed.completed_code))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
